{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning with PySyft and PyTorch - SMS spam detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is an example of usage of [PySyft](https://github.com/OpenMined/PySyft), an open-source library built for Federate Learning and Privacy Preserving. PySyft allows its users to perform private and secure Deep Learning. It is built as an extension of some DL libraries, such as PyTorch, Keras and Tensorflow.\n",
    "\n",
    "In this project I will show how we can use Federated Learning with the PyTorch extension of PySyft for a classification task with a simple 1-layer GRU. \n",
    "\n",
    "The data used for this project was the [SMS Spam Collection Data Set](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection) available on the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php). The dataset consists of c. 5500 SMS messages, of which around 13% are spam messages.\n",
    "\n",
    "The objective here is to simulate two remote machines (that we will call Bob and Anne), where each machine have a similar number of labeled datapoints (SMS labeled as spam or not). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are most interested in the usage of PySyft and Federated Learning, I will skip the text-preprocessing part of the project. If you are interested in how I performed the preprocessing of the raw dataset you can take a look on the script [preprocess.py](https://github.com/andrelmfarias/Private-AI/blob/master/Federated_Learning/preprocess.py) available on my [Github page](https://github.com/andrelmfarias/Private-AI).\n",
    "\n",
    "Each data point of the `inputs.npy` dataset correspond to an array of 30 tokens obtained form each message (padded at left or truncated at right)\n",
    "\n",
    "The `label.npy` dataset has the following unique values: `1` for `spam` and `0` for `non-spam`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T19:33:40.160008Z",
     "start_time": "2019-06-03T19:33:39.344527Z"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.6.5 ('pysyft-fl3')' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n pysyft-fl3 ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T19:33:40.172746Z",
     "start_time": "2019-06-03T19:33:40.163793Z"
    }
   },
   "outputs": [],
   "source": [
    "inputs = np.load('/home/ubuntu/fedLning/pysyft/data/inputs.npy')\n",
    "# /home/ubuntu/fedLning/pysyft/data/inputs.npy\n",
    "labels = np.load('/home/ubuntu/fedLning/pysyft/data/labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T19:33:40.182467Z",
     "start_time": "2019-06-03T19:33:40.176346Z"
    }
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = int(inputs.max()) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model with Federated learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and model hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T19:33:40.207763Z",
     "start_time": "2019-06-03T19:33:40.201292Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training params\n",
    "EPOCHS = 15\n",
    "CLIP = 5 # gradient clipping - to avoid gradient explosion (frequent in RNNs)\n",
    "lr = 0.1\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Model params\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM = 10\n",
    "DROPOUT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Arguments():\n",
    "#     def __init__(self):\n",
    "#         # self.batch_size = 64\n",
    "#         # self.test_batch_size = 1000\n",
    "#         # self.epochs = 10\n",
    "#         # self.lr = 0.01\n",
    "#         # self.momentum = 0.5\n",
    "#         self.no_cuda = False\n",
    "#         self.seed = 1\n",
    "#         # self.log_interval = 30\n",
    "#         # self.save_model = True\n",
    "#         # Training params\n",
    "#         self.EPOCHS = 15\n",
    "#         self.CLIP = 5 # gradient clipping - to avoid gradient explosion (frequent in RNNs)\n",
    "#         self.lr = 0.1\n",
    "#         self.BATCH_SIZE = 32\n",
    "\n",
    "#         # Model params\n",
    "#         self.EMBEDDING_DIM = 50\n",
    "#         self.HIDDEN_DIM = 10\n",
    "#         self.DROPOUT = 0.2\n",
    "\n",
    "# args = Arguments()\n",
    "\n",
    "# use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "# torch.manual_seed(args.seed)\n",
    "\n",
    "# device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "# kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiating virtual workers with Pysyft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we are going to separate the dataset in training and test sets following the ratio 80/20. Each of these datasets will be split in two and will be sent to \"Bob's\" and \"Anne's\" machines in order to **simulate remote and private data**.\n",
    "\n",
    "Pleae note that in a real case, such datasets will be already in the remote machines and the preprocessing will be performed before hand by their own devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T19:33:41.963634Z",
     "start_time": "2019-06-03T19:33:40.212236Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Descriptors cannot not be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msyft\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msy\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pysyft-fl5/lib/python3.8/site-packages/syft/__init__.py:15\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msyft\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mversion\u001b[39;00m \u001b[39mimport\u001b[39;00m __version__\n\u001b[1;32m     13\u001b[0m \u001b[39m# This import statement is strictly here to trigger registration of syft\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39m# tensor types inside hook_args.py.\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msyft\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframeworks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhook\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhook_args\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlogging\u001b[39;00m\n\u001b[1;32m     19\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m__name__\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pysyft-fl5/lib/python3.8/site-packages/syft/frameworks/torch/hook/hook_args.py:6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msyft\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframeworks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtensors\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdecorators\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlogging\u001b[39;00m \u001b[39mimport\u001b[39;00m LoggingTensor\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msyft\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframeworks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtensors\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minterpreters\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpaillier\u001b[39;00m \u001b[39mimport\u001b[39;00m PaillierTensor\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msyft\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframeworks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtensors\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minterpreters\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnative\u001b[39;00m \u001b[39mimport\u001b[39;00m TorchTensor\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msyft\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgeneric\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframeworks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhook\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhook_args\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      8\u001b[0m     get_child,\n\u001b[1;32m      9\u001b[0m     register_ambiguous_method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     one,\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msyft\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m PureFrameworkTensorFoundError\n",
      "File \u001b[0;32m~/anaconda3/envs/pysyft-fl5/lib/python3.8/site-packages/syft/frameworks/torch/tensors/interpreters/native.py:13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msyft\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgeneric\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframeworks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhook\u001b[39;00m \u001b[39mimport\u001b[39;00m hook_args\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msyft\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgeneric\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframeworks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moverload\u001b[39;00m \u001b[39mimport\u001b[39;00m overloaded\n\u001b[0;32m---> 13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msyft\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframeworks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtensors\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minterpreters\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcrt_precision\u001b[39;00m \u001b[39mimport\u001b[39;00m _moduli_for_fields\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msyft\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframeworks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtensors\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minterpreters\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpaillier\u001b[39;00m \u001b[39mimport\u001b[39;00m PaillierTensor\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msyft\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgeneric\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframeworks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtypes\u001b[39;00m \u001b[39mimport\u001b[39;00m FrameworkTensor\n",
      "File \u001b[0;32m~/anaconda3/envs/pysyft-fl5/lib/python3.8/site-packages/syft/frameworks/torch/tensors/interpreters/crt_precision.py:7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mitertools\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msyft\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msyft\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframeworks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtensors\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minterpreters\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprecision\u001b[39;00m \u001b[39mimport\u001b[39;00m FixedPrecisionTensor\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msyft\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgeneric\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframeworks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moverload\u001b[39;00m \u001b[39mimport\u001b[39;00m overloaded\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msyft\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgeneric\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframeworks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhook\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhook_args\u001b[39;00m \u001b[39mimport\u001b[39;00m default_register_tensor\n",
      "File \u001b[0;32m~/anaconda3/envs/pysyft-fl5/lib/python3.8/site-packages/syft/frameworks/torch/tensors/interpreters/precision.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msyft\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msyft\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframeworks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtensors\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minterpreters\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39madditive_shared\u001b[39;00m \u001b[39mimport\u001b[39;00m AdditiveSharingTensor\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msyft\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgeneric\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframeworks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhook\u001b[39;00m \u001b[39mimport\u001b[39;00m hook_args\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msyft\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgeneric\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframeworks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moverload\u001b[39;00m \u001b[39mimport\u001b[39;00m overloaded\n",
      "File \u001b[0;32m~/anaconda3/envs/pysyft-fl5/lib/python3.8/site-packages/syft/frameworks/torch/tensors/interpreters/additive_shared.py:12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msyft\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgeneric\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframeworks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moverload\u001b[39;00m \u001b[39mimport\u001b[39;00m overloaded\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msyft\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mworkers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mabstract\u001b[39;00m \u001b[39mimport\u001b[39;00m AbstractWorker\n\u001b[0;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msyft_proto\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframeworks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtensors\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minterpreters\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv1\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39madditive_shared_pb2\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     13\u001b[0m     AdditiveSharingTensor \u001b[39mas\u001b[39;00m AdditiveSharingTensorPB,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msyft_proto\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtypes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msyft\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv1\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mid_pb2\u001b[39;00m \u001b[39mimport\u001b[39;00m Id \u001b[39mas\u001b[39;00m IdPB\n\u001b[1;32m     17\u001b[0m no_wrap \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mno_wrap\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mTrue\u001b[39;00m}\n",
      "File \u001b[0;32m~/anaconda3/envs/pysyft-fl5/lib/python3.8/site-packages/syft_proto/frameworks/torch/tensors/interpreters/v1/additive_shared_pb2.py:14\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39m# @@protoc_insertion_point(imports)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m _sym_db \u001b[39m=\u001b[39m _symbol_database\u001b[39m.\u001b[39mDefault()\n\u001b[0;32m---> 14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msyft_proto\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgeneric\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpointers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv1\u001b[39;00m \u001b[39mimport\u001b[39;00m pointer_tensor_pb2 \u001b[39mas\u001b[39;00m syft__proto_dot_generic_dot_pointers_dot_v1_dot_pointer__tensor__pb2\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msyft_proto\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtypes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msyft\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv1\u001b[39;00m \u001b[39mimport\u001b[39;00m id_pb2 \u001b[39mas\u001b[39;00m syft__proto_dot_types_dot_syft_dot_v1_dot_id__pb2\n\u001b[1;32m     18\u001b[0m DESCRIPTOR \u001b[39m=\u001b[39m _descriptor\u001b[39m.\u001b[39mFileDescriptor(\n\u001b[1;32m     19\u001b[0m   name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msyft_proto/frameworks/torch/tensors/interpreters/v1/additive_shared.proto\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     20\u001b[0m   package\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msyft_proto.frameworks.torch.tensors.interpreters.v1\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m   ,\n\u001b[1;32m     25\u001b[0m   dependencies\u001b[39m=\u001b[39m[syft__proto_dot_generic_dot_pointers_dot_v1_dot_pointer__tensor__pb2\u001b[39m.\u001b[39mDESCRIPTOR,syft__proto_dot_types_dot_syft_dot_v1_dot_id__pb2\u001b[39m.\u001b[39mDESCRIPTOR,])\n",
      "File \u001b[0;32m~/anaconda3/envs/pysyft-fl5/lib/python3.8/site-packages/syft_proto/generic/pointers/v1/pointer_tensor_pb2.py:14\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39m# @@protoc_insertion_point(imports)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m _sym_db \u001b[39m=\u001b[39m _symbol_database\u001b[39m.\u001b[39mDefault()\n\u001b[0;32m---> 14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msyft_proto\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtypes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msyft\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv1\u001b[39;00m \u001b[39mimport\u001b[39;00m id_pb2 \u001b[39mas\u001b[39;00m syft__proto_dot_types_dot_syft_dot_v1_dot_id__pb2\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msyft_proto\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtypes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msyft\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv1\u001b[39;00m \u001b[39mimport\u001b[39;00m shape_pb2 \u001b[39mas\u001b[39;00m syft__proto_dot_types_dot_syft_dot_v1_dot_shape__pb2\n\u001b[1;32m     18\u001b[0m DESCRIPTOR \u001b[39m=\u001b[39m _descriptor\u001b[39m.\u001b[39mFileDescriptor(\n\u001b[1;32m     19\u001b[0m   name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msyft_proto/generic/pointers/v1/pointer_tensor.proto\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     20\u001b[0m   package\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msyft_proto.generic.pointers.v1\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m   ,\n\u001b[1;32m     25\u001b[0m   dependencies\u001b[39m=\u001b[39m[syft__proto_dot_types_dot_syft_dot_v1_dot_id__pb2\u001b[39m.\u001b[39mDESCRIPTOR,syft__proto_dot_types_dot_syft_dot_v1_dot_shape__pb2\u001b[39m.\u001b[39mDESCRIPTOR,])\n",
      "File \u001b[0;32m~/anaconda3/envs/pysyft-fl5/lib/python3.8/site-packages/syft_proto/types/syft/v1/id_pb2.py:34\u001b[0m\n\u001b[1;32m     11\u001b[0m _sym_db \u001b[39m=\u001b[39m _symbol_database\u001b[39m.\u001b[39mDefault()\n\u001b[1;32m     16\u001b[0m DESCRIPTOR \u001b[39m=\u001b[39m _descriptor\u001b[39m.\u001b[39mFileDescriptor(\n\u001b[1;32m     17\u001b[0m   name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msyft_proto/types/syft/v1/id.proto\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     18\u001b[0m   package\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msyft_proto.types.syft.v1\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m   serialized_pb\u001b[39m=\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m!syft_proto/types/syft/v1/id.proto\u001b[39m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\x18\u001b[39;00m\u001b[39msyft_proto.types.syft.v1\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m<\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x02\u001b[39;00m\u001b[39mId\u001b[39m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\x17\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x06\u001b[39;00m\u001b[39mid_int\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x01\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\x03\u001b[39;00m\u001b[39mH\u001b[39m\u001b[39m\\x00\u001b[39;00m\u001b[39mR\u001b[39m\u001b[39m\\x05\u001b[39;00m\u001b[39midInt\u001b[39m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\x17\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x06\u001b[39;00m\u001b[39mid_str\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x02\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39mH\u001b[39m\u001b[39m\\x00\u001b[39;00m\u001b[39mR\u001b[39m\u001b[39m\\x05\u001b[39;00m\u001b[39midStrB\u001b[39m\u001b[39m\\x04\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x02\u001b[39;00m\u001b[39midB\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m%o\u001b[39;00m\u001b[39mrg.openmined.syftproto.types.syft.v1b\u001b[39m\u001b[39m\\x06\u001b[39;00m\u001b[39mproto3\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     27\u001b[0m _ID \u001b[39m=\u001b[39m _descriptor\u001b[39m.\u001b[39mDescriptor(\n\u001b[1;32m     28\u001b[0m   name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mId\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     29\u001b[0m   full_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msyft_proto.types.syft.v1.Id\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     30\u001b[0m   filename\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     31\u001b[0m   file\u001b[39m=\u001b[39mDESCRIPTOR,\n\u001b[1;32m     32\u001b[0m   containing_type\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     33\u001b[0m   fields\u001b[39m=\u001b[39m[\n\u001b[0;32m---> 34\u001b[0m     _descriptor\u001b[39m.\u001b[39;49mFieldDescriptor(\n\u001b[1;32m     35\u001b[0m       name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mid_int\u001b[39;49m\u001b[39m'\u001b[39;49m, full_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39msyft_proto.types.syft.v1.Id.id_int\u001b[39;49m\u001b[39m'\u001b[39;49m, index\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m     36\u001b[0m       number\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39mtype\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, cpp_type\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, label\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     37\u001b[0m       has_default_value\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, default_value\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m     38\u001b[0m       message_type\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, enum_type\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, containing_type\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     39\u001b[0m       is_extension\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, extension_scope\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     40\u001b[0m       serialized_options\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, json_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39midInt\u001b[39;49m\u001b[39m'\u001b[39;49m, file\u001b[39m=\u001b[39;49mDESCRIPTOR),\n\u001b[1;32m     41\u001b[0m     _descriptor\u001b[39m.\u001b[39mFieldDescriptor(\n\u001b[1;32m     42\u001b[0m       name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mid_str\u001b[39m\u001b[39m'\u001b[39m, full_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msyft_proto.types.syft.v1.Id.id_str\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     43\u001b[0m       number\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39m9\u001b[39m, cpp_type\u001b[39m=\u001b[39m\u001b[39m9\u001b[39m, label\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     44\u001b[0m       has_default_value\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, default_value\u001b[39m=\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m     45\u001b[0m       message_type\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, enum_type\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, containing_type\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     46\u001b[0m       is_extension\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, extension_scope\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     47\u001b[0m       serialized_options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, json_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39midStr\u001b[39m\u001b[39m'\u001b[39m, file\u001b[39m=\u001b[39mDESCRIPTOR),\n\u001b[1;32m     48\u001b[0m   ],\n\u001b[1;32m     49\u001b[0m   extensions\u001b[39m=\u001b[39m[\n\u001b[1;32m     50\u001b[0m   ],\n\u001b[1;32m     51\u001b[0m   nested_types\u001b[39m=\u001b[39m[],\n\u001b[1;32m     52\u001b[0m   enum_types\u001b[39m=\u001b[39m[\n\u001b[1;32m     53\u001b[0m   ],\n\u001b[1;32m     54\u001b[0m   serialized_options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     55\u001b[0m   is_extendable\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     56\u001b[0m   syntax\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mproto3\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     57\u001b[0m   extension_ranges\u001b[39m=\u001b[39m[],\n\u001b[1;32m     58\u001b[0m   oneofs\u001b[39m=\u001b[39m[\n\u001b[1;32m     59\u001b[0m     _descriptor\u001b[39m.\u001b[39mOneofDescriptor(\n\u001b[1;32m     60\u001b[0m       name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m, full_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msyft_proto.types.syft.v1.Id.id\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     61\u001b[0m       index\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, containing_type\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, fields\u001b[39m=\u001b[39m[]),\n\u001b[1;32m     62\u001b[0m   ],\n\u001b[1;32m     63\u001b[0m   serialized_start\u001b[39m=\u001b[39m\u001b[39m63\u001b[39m,\n\u001b[1;32m     64\u001b[0m   serialized_end\u001b[39m=\u001b[39m\u001b[39m123\u001b[39m,\n\u001b[1;32m     65\u001b[0m )\n\u001b[1;32m     67\u001b[0m _ID\u001b[39m.\u001b[39moneofs_by_name[\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mfields\u001b[39m.\u001b[39mappend(\n\u001b[1;32m     68\u001b[0m   _ID\u001b[39m.\u001b[39mfields_by_name[\u001b[39m'\u001b[39m\u001b[39mid_int\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     69\u001b[0m _ID\u001b[39m.\u001b[39mfields_by_name[\u001b[39m'\u001b[39m\u001b[39mid_int\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mcontaining_oneof \u001b[39m=\u001b[39m _ID\u001b[39m.\u001b[39moneofs_by_name[\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/pysyft-fl5/lib/python3.8/site-packages/google/protobuf/descriptor.py:561\u001b[0m, in \u001b[0;36mFieldDescriptor.__new__\u001b[0;34m(cls, name, full_name, index, number, type, cpp_type, label, default_value, message_type, enum_type, containing_type, is_extension, extension_scope, options, serialized_options, has_default_value, containing_oneof, json_name, file, create_key)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__new__\u001b[39m(\u001b[39mcls\u001b[39m, name, full_name, index, number, \u001b[39mtype\u001b[39m, cpp_type, label,\n\u001b[1;32m    556\u001b[0m             default_value, message_type, enum_type, containing_type,\n\u001b[1;32m    557\u001b[0m             is_extension, extension_scope, options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    558\u001b[0m             serialized_options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    559\u001b[0m             has_default_value\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, containing_oneof\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, json_name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    560\u001b[0m             file\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, create_key\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):  \u001b[39m# pylint: disable=redefined-builtin\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m   _message\u001b[39m.\u001b[39;49mMessage\u001b[39m.\u001b[39;49m_CheckCalledFromGeneratedFile()\n\u001b[1;32m    562\u001b[0m   \u001b[39mif\u001b[39;00m is_extension:\n\u001b[1;32m    563\u001b[0m     \u001b[39mreturn\u001b[39;00m _message\u001b[39m.\u001b[39mdefault_pool\u001b[39m.\u001b[39mFindExtensionByName(full_name)\n",
      "\u001b[0;31mTypeError\u001b[0m: Descriptors cannot not be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates"
     ]
    }
   ],
   "source": [
    "import syft as sy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T19:33:40.197935Z",
     "start_time": "2019-06-03T19:33:40.186270Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = torch.tensor(labels)\n",
    "inputs = torch.tensor(inputs)\n",
    "\n",
    "# splitting training and test data\n",
    "pct_test = 0.2\n",
    "\n",
    "train_labels = labels[:-int(len(labels)*pct_test)]\n",
    "train_inputs = inputs[:-int(len(labels)*pct_test)]\n",
    "\n",
    "test_labels = labels[-int(len(labels)*pct_test):]\n",
    "test_inputs = inputs[-int(len(labels)*pct_test):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T19:33:42.591430Z",
     "start_time": "2019-06-03T19:33:41.969220Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hook that extends the Pytorch library to enable all computations with pointers of tensors sent to other workers\n",
    "hook = sy.TorchHook(torch)\n",
    "\n",
    "# Creating 2 virtual workers\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")\n",
    "anne = sy.VirtualWorker(hook, id=\"anne\")\n",
    "\n",
    "# threshold indexes for dataset split (one half for Bob, other half for Anne)\n",
    "train_idx = int(len(train_labels)/2)\n",
    "test_idx = int(len(test_labels)/2)\n",
    "\n",
    "# Sending toy datasets to virtual workers\n",
    "bob_train_dataset = sy.BaseDataset(train_inputs[:train_idx], train_labels[:train_idx]).send(bob)\n",
    "anne_train_dataset = sy.BaseDataset(train_inputs[train_idx:], train_labels[train_idx:]).send(anne)\n",
    "bob_test_dataset = sy.BaseDataset(test_inputs[:test_idx], test_labels[:test_idx]).send(bob)\n",
    "anne_test_dataset = sy.BaseDataset(test_inputs[test_idx:], test_labels[test_idx:]).send(anne)\n",
    "\n",
    "# Creating federated datasets, an extension of Pytorch TensorDataset class\n",
    "federated_train_dataset = sy.FederatedDataset([bob_train_dataset, anne_train_dataset])\n",
    "federated_test_dataset = sy.FederatedDataset([bob_test_dataset, anne_test_dataset])\n",
    "\n",
    "# Creating federated dataloaders, an extension of Pytorch DataLoader class\n",
    "federated_train_loader = sy.FederatedDataLoader(federated_train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
    "federated_test_loader = sy.FederatedDataLoader(federated_test_dataset, shuffle=False, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating simple GRU (1-layer) model with sigmoid activation for classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunatelly, the current version of PySyft does not support the RNNs modules of PyTorch yet. However, I was able to handcraft a simple GRU network with linear layers for this project. \n",
    "\n",
    "As the focus of this notebook is the usage of PySyft, I will skip the construction of the model. If you are interested in how I built the model, you can check it out on [handcrafted_GRU.py](https://github.com/andrelmfarias/Private-AI/blob/master/Federated_Learning/handcrafted_GRU.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T19:33:42.613017Z",
     "start_time": "2019-06-03T19:33:42.598004Z"
    }
   },
   "outputs": [],
   "source": [
    "from handcrafted_GRU import GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T19:33:42.638046Z",
     "start_time": "2019-06-03T19:33:42.617601Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initiating the model\n",
    "model = GRU(vocab_size=VOCAB_SIZE, hidden_dim=HIDDEN_DIM, embedding_dim=EMBEDDING_DIM, dropout=DROPOUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, PySyft does not support optimizers with momentum. Therefore, we are going to stick with the classical [Stochastic Gradient Descent](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD) optimizer.\n",
    "\n",
    "As our task consists of a binary classification, we are going to use the [Binary Cross Entropy Loss](https://pytorch.org/docs/stable/nn.html#torch.nn.BCELoss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T20:00:23.084933Z",
     "start_time": "2019-06-03T20:00:23.078688Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each epoch we are going to compute the training and validations losses, as well as the [Area Under the ROC Curve](https://scikit-learn.org/stable/modules/model_evaluation.html#roc-metrics) score due to the fact that the target dataset is unbalaced (only 13% of labels are positive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T19:56:01.459697Z",
     "start_time": "2019-06-03T19:33:42.666174Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "for e in range(EPOCHS):\n",
    "    \n",
    "    ######### Training ##########\n",
    "    \n",
    "    losses = []\n",
    "    # Batch loop\n",
    "    for inputs, labels in federated_train_loader:\n",
    "        # Location of current batch\n",
    "        worker = inputs.location\n",
    "        # Initialize hidden state and send it to worker\n",
    "        h = torch.Tensor(np.zeros((BATCH_SIZE, HIDDEN_DIM))).send(worker)\n",
    "        # Send model to current worker\n",
    "        model.send(worker)\n",
    "\n",
    "        # This is a GPU speed up process\n",
    "        # inputs,labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Setting accumulated gradients to zero before backward step\n",
    "        optimizer.zero_grad()\n",
    "        # Output from the model\n",
    "        output, _ = model(inputs, h)\n",
    "        # Calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        # Clipping the gradient to avoid explosion\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "        # Backpropagation step\n",
    "        optimizer.step() \n",
    "        # Get the model back to the local worker\n",
    "        model.get()\n",
    "        losses.append(loss.get())\n",
    "    \n",
    "    ######## Evaluation ##########\n",
    "    \n",
    "    # Model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_preds = []\n",
    "        test_labels_list = []\n",
    "        eval_losses = []\n",
    "\n",
    "        for inputs, labels in federated_test_loader:\n",
    "            # get current location\n",
    "            worker = inputs.location\n",
    "            # Initialize hidden state and send it to worker\n",
    "            h = torch.Tensor(np.zeros((BATCH_SIZE, HIDDEN_DIM))).send(worker)   \n",
    "            # Send model to worker\n",
    "            model.send(worker)\n",
    "            \n",
    "            # h, model = h.to(device), model.to(device)\n",
    "            \n",
    "            output, _ = model(inputs, h)\n",
    "            loss = criterion(output.squeeze(), labels.float())\n",
    "            eval_losses.append(loss.get())\n",
    "            preds = output.squeeze().get()\n",
    "            test_preds += list(preds.numpy())\n",
    "            test_labels_list += list(labels.get().numpy().astype(int))\n",
    "            # Get the model back to the local worker\n",
    "            model.get()\n",
    "        \n",
    "        score = roc_auc_score(test_labels_list, test_preds)\n",
    "    \n",
    "    print(\"Epoch {}/{}...  \\\n",
    "    AUC: {:.3%}...  \\\n",
    "    Training loss: {:.5f}...  \\\n",
    "    Validation loss: {:.5f}\".format(e+1, EPOCHS, score, sum(losses)/len(losses), sum(eval_losses)/len(eval_losses)))\n",
    "    \n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that with the PySyft library and its PyTorch extension, we can perform operations with tensor pointers such as we can do with PyTorch API (but for some limitations that are still to be addressed). \n",
    "\n",
    "Thanks to this, we were able to train spam detector model without having any access to the remote and private data: for each batch we sent the model to the current remote worker and got it back to the local machine before sending it to the worker of the next batch.\n",
    "\n",
    "We can also notice that this federated training did not harm the performance of the model as both losses reduced at each epoch as expected and the final AUC score on the test data was above 97.5%.\n",
    "\n",
    "There is however one limitation of this method: by getting the model back we can still have access to some private information. \n",
    "Let's say Bob had only one SMS on his machine. When we get the model back, we can just check which embeddings of the model changed and we will know which were the tokens (words) of the SMS.\n",
    "\n",
    "In order to address this issue, there are two solutions: Differential Privacy and Secured Multi-Party Computation (SMPC). Differential Privacy would be used to make sure the model does not give access to some private information. SMPC, which is one kind of Encrypted Computation, in return allows you to send the model privately so that the remote workers which have the data cannot see the weights you are using.\n",
    "\n",
    "I will show how can we perform these techniques with PySyft in a next tutorial.\n",
    "\n",
    "Et voilà!"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3.6.5 ('pysyft-fl3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "f6634fcd72fb132451643f5f55a02cfe3294ff82b3d7a98b6a6d89ccc08de9bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
